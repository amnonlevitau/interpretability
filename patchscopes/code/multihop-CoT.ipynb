{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJYfysaREkb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mdEmY4rDQ3ik",
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T22:12:46.745345400Z",
     "start_time": "2024-06-25T22:12:45.719925500Z"
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Scienfitic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import datasets\n",
    "from torch import cuda\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Visuals\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(context=\"notebook\",\n",
    "        rc={\"font.size\": 16,\n",
    "            \"axes.titlesize\": 16,\n",
    "            \"axes.labelsize\": 16,\n",
    "            \"xtick.labelsize\": 16.0,\n",
    "            \"ytick.labelsize\": 16.0,\n",
    "            \"legend.fontsize\": 16.0})\n",
    "palette_ = sns.color_palette(\"Set1\")\n",
    "palette = palette_[2:5] + palette_[7:]\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Utilities\n",
    "\n",
    "from general_utils import (\n",
    "    ModelAndTokenizer,\n",
    "    make_inputs,\n",
    "    decode_tokens,\n",
    "    find_token_range,\n",
    "    predict_from_input,\n",
    ")\n",
    "\n",
    "from patchscopes_utils import *\n",
    "\n",
    "import tqdm\n",
    "\n",
    "tqdm.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oURHfJrzap1H",
    "ExecuteTime": {
     "end_time": "2024-06-25T22:14:25.171435800Z",
     "start_time": "2024-06-25T22:12:46.747345900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "decd37de85344b449d4f6250b46ef0e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n    (layers): ModuleList(\n      (0): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (1): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (2): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (3): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (4): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (5): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (6): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (7): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (8): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (9): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (10): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (11): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (12): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (13): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (14): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (15): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (16): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (17): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (18): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (19): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (20): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (21): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (22): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (23): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (24): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (25): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (26): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (27): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (28): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (29): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (30): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n      (31): LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size_scale = 4\n",
    "# Load model\n",
    "\n",
    "model_name = \"lmsys/vicuna-7b-v1.1\"\n",
    "torch_dtype = torch.float16\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# torch_dtype = torch.bfloat16\n",
    "\n",
    "sos_tok = False\n",
    "\n",
    "\n",
    "my_device = torch.device(\"cuda:0\")\n",
    "\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=my_device,\n",
    ")\n",
    "mt.set_hs_patch_hooks = set_hs_patch_hooks_llama_batch\n",
    "mt.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAtiySLToTY7"
   },
   "source": [
    "# MultiHop reasoning experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PfhXcB54ap1I",
    "ExecuteTime": {
     "end_time": "2024-06-25T22:14:25.181939700Z",
     "start_time": "2024-06-25T22:14:25.169434900Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_baseline_multihop(\n",
    "        mt, df, batch_size=256 // batch_size_scale, max_gen_len=10,\n",
    "):\n",
    "    def _generate_baseline_single_batch(batch_df):\n",
    "        batch_size = len(batch_df)\n",
    "        cases = [(\"baseline_hop2\", \"hop2\"),\n",
    "                 (\"baseline_hop3\", \"hop3\"),\n",
    "                 (\"baseline_multihop3\", \"hop3\"),\n",
    "                 ]\n",
    "        results = {}\n",
    "        for target_col, object_col in cases:\n",
    "            target_baseline_batch = np.array(batch_df[target_col])\n",
    "            object_batch = np.array(batch_df[object_col])\n",
    "\n",
    "            # Step 0: run the the model on target prompt baseline (having the subject token in input rather than patched)\n",
    "            # The goal of this step is to calculate whether the model works correctly by default, and to calculate surprisal\n",
    "            inp_target_baseline = make_inputs(mt.tokenizer, target_baseline_batch, mt.device)\n",
    "            seq_len_target_baseline = len(inp_target_baseline[\"input_ids\"][0])\n",
    "            output_target_baseline_toks = mt.model.generate(\n",
    "                inp_target_baseline[\"input_ids\"],\n",
    "                max_length=seq_len_target_baseline + max_gen_len,\n",
    "                pad_token_id=mt.model.generation_config.eos_token_id,\n",
    "            )[:, seq_len_target_baseline:]\n",
    "            generations_baseline = decode_tokens(mt.tokenizer, output_target_baseline_toks)\n",
    "            generations_baseline_txt = np.array([\" \".join(sample_gen) for sample_gen in generations_baseline])\n",
    "\n",
    "            is_correct_baseline = np.array([\n",
    "                (object_batch[i] in generations_baseline_txt[i] or\n",
    "                 object_batch[i].replace(\" \", \"\") in generations_baseline_txt[i].replace(\" \", \"\"))\n",
    "                for i in range(batch_size)\n",
    "            ])\n",
    "            results.update(\n",
    "                {\n",
    "                    f\"generations_{target_col}\": generations_baseline_txt,\n",
    "                    f\"is_correct_{target_col}\": is_correct_baseline,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "    results = {}\n",
    "    n_batches = len(df) // batch_size\n",
    "    if len(df) % batch_size != 0:\n",
    "        n_batches += 1\n",
    "    for i in tqdm.tqdm(range(n_batches)):\n",
    "        cur_df = df.iloc[batch_size * i: batch_size * (i + 1)]\n",
    "        batch_results = _generate_baseline_single_batch(cur_df)\n",
    "        for key, value in batch_results.items():\n",
    "            if key in results:\n",
    "                results[key] = np.concatenate((results[key], value))\n",
    "            else:\n",
    "                results[key] = value\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3YQSI-MoTY8"
   },
   "source": [
    "# Experiment 1: Multihop Product Company CEO tuples\n",
    "\n",
    "This is a subset made only from (product, company) and (company, CEO) tuples from the LRE dataset.\n",
    "We only picked 3 (company, CEO) tuples, and 15 (product, company) tuples for each that the model is more likely to know the answer to.\n",
    "\n",
    "This is an exploratory experiment. There is a more complete experiment later in the colab.\n",
    "Hop 1: Product\n",
    "Hop 2: company\n",
    "Hop 3: CEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fW9dio43ap1J",
    "ExecuteTime": {
     "end_time": "2024-06-25T22:14:25.225035200Z",
     "start_time": "2024-06-25T22:14:25.183939800Z"
    }
   },
   "outputs": [],
   "source": [
    "multihop_samples = {\n",
    "    (\"Satya Nadella\", \"Microsoft\"): [\"WinDbg\", \".NET Framework\", \"Internet Explorer\", \"MS-DOS\", \"Office Open XML\",\n",
    "                                     \"TypeScript\", \"Bing Maps Platform\", \"Outlook Express\", \"PowerShell\", \"Windows 95\",\n",
    "                                     \"Xbox 360\", \"Zune\", \"Visual Basic Script\", \"Virtual Hard Disk\", \"Robocopy\",\n",
    "                                     ],\n",
    "    (\"Tim Cook\", \"Apple\"): [\"Siri\", \"App Store\", \"CarPlay\", \"MacBook Air\", \"Xcode\",\n",
    "                            \"macOS\", \"iWork\", \"Safari\", \"QuickTime\", \"TextEdit\",\n",
    "                            \"WebKit\", \"QuickDraw\", \"Time Machine (macOS)\", \"MessagePad\", \"Macbook Pro\",\n",
    "                            ],\n",
    "    (\"Sundar Pichai\", \"Google\"): [\"Chromecast\", \"Chromebook\", \"Wear OS\", \"G Suite\", \"Picasa\",\n",
    "                                  \"WebP Lossless\", \"General Transit Feed Specification Lossless\", \"Cloud Spanner\",\n",
    "                                  \"Android TV\", \"Android Runtime\",\n",
    "                                  \"Android Jelly Bean\", \"Android Auto\", \"App Inventor\", \"Chromebook Pixel\",\n",
    "                                  \"Project Ara\",\n",
    "                                  ]\n",
    "}\n",
    "\n",
    "\n",
    "def generate_multihop_data_ceo(fdir_out=\"./outputs/factual\", batch_size=512 // batch_size_scale, max_gen_len=20,\n",
    "                               replace=False):\n",
    "    if not os.path.exists(fdir_out):\n",
    "        os.makedirs(fdir_out)\n",
    "    fname_out = \"multihop_product_company_ceo\"\n",
    "    if not replace and os.path.exists(f\"{fdir_out}/{fname_out}.pkl\"):\n",
    "        print(f\"File {fdir_out}/{fname_out}.pkl exists. Skipping generation. Reading file...\")\n",
    "        df = pd.read_pickle(os.path.join(fdir_out, f\"{fname_out}.pkl\"))\n",
    "        return df\n",
    "    prompt_source_template = \"{} was created by\"\n",
    "    prompt_target_template = \"Who is the current CEO of {}\"\n",
    "    sample_id = 0\n",
    "\n",
    "    print(\"Step 1: Prepare dataset...\")\n",
    "    records = []\n",
    "\n",
    "    for key, value in multihop_samples.items():\n",
    "        hop3, hop2 = key\n",
    "        for hop1 in value:\n",
    "            # hop1: Product\n",
    "            # hop2: Company\n",
    "            # hop3: CEO\n",
    "            records.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"prompt_source\": prompt_source_template.replace(\"{}\", hop1),\n",
    "                \"position_source\": -1,  # always doing next token prediction\n",
    "                \"prompt_target\": prompt_target_template,\n",
    "                \"position_target\": -1,\n",
    "\n",
    "                \"baseline_hop2\": f\"{hop1} was created by\",  #  hop2\n",
    "                \"baseline_hop3\": f\"Who is the current CEO of {hop2}\",  # hop3\n",
    "                \"baseline_multihop3\": f\"Who is the current CEO of the company that created {hop1}\",  # hop3\n",
    "\n",
    "                \"hop1\": hop1,\n",
    "                \"hop2\": hop2,\n",
    "                \"hop3\": hop3,\n",
    "            })\n",
    "            sample_id += 1\n",
    "\n",
    "    # Step 2: Compute baseline generations\n",
    "    print(\"Step 2: Compute baseline generations...\")\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    eval_results = generate_baseline_multihop(mt, df, batch_size=batch_size, max_gen_len=max_gen_len)\n",
    "    for key, value in eval_results.items():\n",
    "        df[key] = list(value)\n",
    "\n",
    "    df.to_csv(os.path.join(fdir_out, f\"{fname_out}.tsv\"), sep=\"\\t\")\n",
    "    df.to_pickle(os.path.join(fdir_out, f\"{fname_out}.pkl\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0Ax2wLmxap1J",
    "ExecuteTime": {
     "end_time": "2024-06-25T22:14:25.230035700Z",
     "start_time": "2024-06-25T22:14:25.196517500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./outputs/factual/multihop_product_company_ceo.pkl exists. Skipping generation. Reading file...\n"
     ]
    }
   ],
   "source": [
    "multihop_df = generate_multihop_data_ceo(batch_size=128 // batch_size_scale, max_gen_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0uklW1xTap1K",
    "ExecuteTime": {
     "end_time": "2024-06-26T17:00:35.521164600Z",
     "start_time": "2024-06-26T17:00:35.502679900Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_attriburte_exraction_batch_multihop(\n",
    "        mt, df, batch_size=256 // batch_size_scale, max_gen_len=10, transform=None, patch_count=1\n",
    "):\n",
    "    def _evaluate_attriburte_exraction_single_batch(batch_df):\n",
    "        batch_size = len(batch_df)\n",
    "        prompt_source_batch = np.array(batch_df[\"prompt_source\"])\n",
    "        prompt_target_batch = np.array(batch_df[\"prompt_target\"])\n",
    "        \n",
    "        if \"layer_sources\" in batch_df:\n",
    "            layer_sources_batch = np.array(batch_df[\"layer_sources\"])\n",
    "            layer_targets_batch = np.array(batch_df[\"layer_targets\"])\n",
    "        else:\n",
    "            # backwards compatibility\n",
    "            layer_sources_batch = np.expand_dims(np.array(batch_df[\"layer_source\"]), -1)\n",
    "            layer_targets_batch = np.expand_dims(np.array(batch_df[\"layer_target\"]), -1)\n",
    "            \n",
    "        if \"position_sources\" in batch_df:\n",
    "            position_sources_batch = batch_df[\"position_sources\"]\n",
    "            position_targets_batch = batch_df[\"position_targets\"]\n",
    "        else:\n",
    "            position_sources_batch = np.expand_dims(np.array(batch_df[\"position_source\"]), -1)\n",
    "            position_targets_batch = np.expand_dims(np.array(batch_df[\"position_target\"]), -1)\n",
    "\n",
    "        object_batch = np.array(batch_df[\"hop3\"])\n",
    "\n",
    "        # Adjust position_target to be absolute rather than relative\n",
    "        inp_target = make_inputs(mt.tokenizer, prompt_target_batch, mt.device)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(patch_count):\n",
    "                if position_targets_batch[i][j] < 0:\n",
    "                    position_targets_batch[i][j] += len(inp_target[\"input_ids\"][j])\n",
    "\n",
    "        # Step 1: run the the model on source without patching and get the hidden representations.\n",
    "        inp_source = make_inputs(mt.tokenizer, prompt_source_batch, mt.device)\n",
    "        output_orig = mt.model(**inp_source, output_hidden_states=True)\n",
    "\n",
    "        # hidden_states size (n_layers, n_sample, seq_len, hidden_dim)\n",
    "        hidden_reps = [\n",
    "            [\n",
    "                output_orig.hidden_states[layer_sources_batch[i][j] + 1][i][position_sources_batch[i][j]]\n",
    "                for i in range(batch_size)\n",
    "            ]\n",
    "            for j in range(patch_count)\n",
    "        ]\n",
    "        if transform is not None:\n",
    "            for i in range(patch_count):\n",
    "                for j in range(batch_size):\n",
    "                    hidden_reps[i][j] = transform(hidden_reps[i][j])\n",
    "        \n",
    "        # Step 2: do second run on target prompt, while patching the input hidden state.\n",
    "        hs_patch_configs = [\n",
    "            [\n",
    "                {\n",
    "                    \"batch_idx\": i,\n",
    "                    \"layer_target\": layer_targets_batch[i][j],\n",
    "                    \"position_target\": position_targets_batch[i][j],\n",
    "                    \"hidden_rep\": hidden_reps[j][i],  # supposed to be reversed indices\n",
    "                    \"skip_final_ln\": (\n",
    "                            layer_sources_batch[i][j]\n",
    "                            == layer_targets_batch[i][j]\n",
    "                            == mt.num_layers - 1\n",
    "                    ),\n",
    "                }\n",
    "                for i in range(batch_size)\n",
    "            ]\n",
    "            for j in range(patch_count) \n",
    "        ]\n",
    "        patch_hooks_list = [\n",
    "            mt.set_hs_patch_hooks(mt.model, hs_patch_config, patch_input=False, generation_mode=True)\n",
    "            for hs_patch_config in hs_patch_configs\n",
    "        ]\n",
    "\n",
    "        output = mt.model(**inp_target)\n",
    "\n",
    "        # NOTE: inputs are left padded,\n",
    "        # and sequence length is the same across batch\n",
    "        seq_len = len(inp_target[\"input_ids\"][0])\n",
    "        output_toks = mt.model.generate(\n",
    "            inp_target[\"input_ids\"],\n",
    "            max_length=seq_len + max_gen_len,\n",
    "            pad_token_id=mt.model.generation_config.eos_token_id,\n",
    "        )[:, seq_len:]\n",
    "        generations_patched = decode_tokens(mt.tokenizer, output_toks)\n",
    "        generations_patched_txt = np.array([\n",
    "            \" \".join(generations_patched[i])\n",
    "            for i in range(batch_size)\n",
    "        ])\n",
    "        is_correct_patched = np.array([\n",
    "            (object_batch[i] in generations_patched_txt[i]\n",
    "             or object_batch[i].replace(\" \", \"\") in generations_patched_txt[i].replace(\" \", \"\"))\n",
    "            for i in range(batch_size)\n",
    "        ])\n",
    "\n",
    "        # remove patching hooks\n",
    "        for patch_hooks in patch_hooks_list:\n",
    "            remove_hooks(patch_hooks)\n",
    "\n",
    "\n",
    "        cpu_hidden_reps = [np.array([hidden_rep[i].detach().cpu().numpy() for i in range(batch_size)]) for hidden_rep in hidden_reps]\n",
    "\n",
    "        results_list = [{\n",
    "            \"generations_patched\": generations_patched,\n",
    "            \"is_correct_patched\": is_correct_patched,\n",
    "            \"hidden_rep\": cpu_hidden_rep,\n",
    "        } for cpu_hidden_rep in cpu_hidden_reps]\n",
    "\n",
    "        return results_list\n",
    "\n",
    "    patch_results = [{}] * patch_count\n",
    "    n_batches = len(df) // batch_size\n",
    "    if len(df) % batch_size != 0:\n",
    "        n_batches += 1\n",
    "    for i in tqdm.tqdm(range(len(df) // batch_size)):\n",
    "        cur_df = df.iloc[batch_size * i: batch_size * (i + 1)]\n",
    "        batch_results = _evaluate_attriburte_exraction_single_batch(cur_df)\n",
    "        for patch_idx in range(patch_count): \n",
    "            for key, value in batch_results[patch_idx].items():\n",
    "                if key in patch_results[patch_idx]:\n",
    "                    patch_results[patch_idx][key] = np.concatenate((patch_results[patch_idx][key], value))\n",
    "                else:\n",
    "                    patch_results[patch_idx][key] = value\n",
    "\n",
    "    return patch_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iWL0cllWap1K",
    "ExecuteTime": {
     "end_time": "2024-06-26T17:00:36.667722300Z",
     "start_time": "2024-06-26T17:00:36.645706400Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(fname_in, fdir_out, fname_out=\"multihop\", batch_size=512 // batch_size_scale, n_samples=-1,\n",
    "                   save_output=True, replace=False, tsv=False, patch_count=1, is_identical_layers=True, is_src_gt_dst=True):\n",
    "    # patch_count is the number of compared entities. in the original experiment, it's 1 and in our case it's 2.\n",
    "    print(f\"Running experiment on {fname_in}...\")\n",
    "    if not replace and os.path.exists(f\"{fdir_out}/{fname_out}.pkl\"):\n",
    "        print(f\"File {fdir_out}/{fname_out}.pkl exists. Skipping generation. Reading file...\")\n",
    "        results_df = pd.read_pickle(f\"{fdir_out}/{fname_out}.pkl\")\n",
    "        return results_df\n",
    "    if tsv:\n",
    "        df = pd.read_csv(f\"{fname_in}\", sep='\\t', header=0)\n",
    "    else:\n",
    "        df = pd.read_pickle(f\"{fname_in}\")\n",
    "    print(f\"\\tNumber of samples: {len(df)}\")\n",
    "\n",
    "    # BATCHing all layers combinations\n",
    "    batch = []\n",
    "    layer_sources= np.arange(mt.num_layers)\n",
    "    layer_targets = np.arange(mt.num_layers)    \n",
    "    if is_identical_layers:\n",
    "        layers_meshed = np.transpose(np.meshgrid(layer_sources, layer_targets)).reshape(-1, 2)\n",
    "        layers_ds = np.expand_dims(layers_meshed, -1).repeat(patch_count, axis=-1)\n",
    "    else: \n",
    "        mesh_list = [layer_sources, layer_targets] * patch_count\n",
    "        layers_ds = np.reshape(np.meshgrid(*mesh_list), (patch_count, 2, -1)).transpose()\n",
    "    if is_src_gt_dst:\n",
    "        # reduces amount by (2*38/37)^2 ~= 4.2\n",
    "        layers_ds = layers_ds[[np.all(layer[0] > layer[1]) for layer in layers_ds]]\n",
    "    #  TODO: support further limitations on src/dst\n",
    "    for _, row in tqdm.tqdm(df.iterrows()):\n",
    "        for layers in layers_ds:\n",
    "                item = dict(row)\n",
    "                item.update({\n",
    "                    \"layer_sources\": layers[0],\n",
    "                    \"layer_targets\": layers[1],\n",
    "                })\n",
    "                if layers[0].shape[0] == 1:\n",
    "                    item.update({\"layer_source\": layers[0]})\n",
    "                if layers[1].shape[0] == 1:\n",
    "                    item.update({\"layer_target\": layers[1]})\n",
    "                    \n",
    "                batch.append(item)\n",
    "    experiment_df = pd.DataFrame.from_records(batch)\n",
    "\n",
    "    if n_samples > 0 and n_samples < len(experiment_df):\n",
    "        experiment_df = experiment_df.sample(n=n_samples, replace=False, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\tNumber of datapoints for patching experiment: {len(experiment_df)}\")\n",
    "\n",
    "    eval_results = evaluate_attriburte_exraction_batch_multihop(mt, experiment_df, batch_size=batch_size, patch_count=patch_count)\n",
    "\n",
    "    eval_results = eval_results[0]  # TODO: add support for the multi-patch \n",
    "    results_df = experiment_df.head(len(eval_results[\"is_correct_patched\"]))\n",
    "    for key, value in eval_results.items():\n",
    "        results_df[key] = list(value)\n",
    "\n",
    "    if save_output:\n",
    "        if not os.path.exists(fdir_out):\n",
    "            os.makedirs(fdir_out)\n",
    "        results_df.to_csv(f\"{fdir_out}/{fname_out}.tsv\", sep=\"\\t\")\n",
    "        results_df.to_pickle(f\"{fdir_out}/{fname_out}.pkl\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9Wlz2wlRap1K",
    "ExecuteTime": {
     "end_time": "2024-06-26T17:18:17.169733600Z",
     "start_time": "2024-06-26T17:00:37.394352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment on ./outputs/factual/multihop_product_company_ceo.pkl...\n",
      "\tNumber of samples: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:01, 42.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of datapoints for patching experiment: 46080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1440/1440 [17:31<00:00,  1.37it/s]\n",
      "C:\\Users\\NickName\\AppData\\Local\\Temp\\ipykernel_34360\\1137992600.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df[key] = list(value)\n",
      "C:\\Users\\NickName\\AppData\\Local\\Temp\\ipykernel_34360\\1137992600.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df[key] = list(value)\n",
      "C:\\Users\\NickName\\AppData\\Local\\Temp\\ipykernel_34360\\1137992600.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  results_df[key] = list(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": "       sample_id               prompt_source  position_source  \\\n0              0       WinDbg was created by               -1   \n1              0       WinDbg was created by               -1   \n2              0       WinDbg was created by               -1   \n3              0       WinDbg was created by               -1   \n4              0       WinDbg was created by               -1   \n...          ...                         ...              ...   \n46075         44  Project Ara was created by               -1   \n46076         44  Project Ara was created by               -1   \n46077         44  Project Ara was created by               -1   \n46078         44  Project Ara was created by               -1   \n46079         44  Project Ara was created by               -1   \n\n                      prompt_target  position_target  \\\n0      Who is the current CEO of {}               -1   \n1      Who is the current CEO of {}               -1   \n2      Who is the current CEO of {}               -1   \n3      Who is the current CEO of {}               -1   \n4      Who is the current CEO of {}               -1   \n...                             ...              ...   \n46075  Who is the current CEO of {}               -1   \n46076  Who is the current CEO of {}               -1   \n46077  Who is the current CEO of {}               -1   \n46078  Who is the current CEO of {}               -1   \n46079  Who is the current CEO of {}               -1   \n\n                    baseline_hop2                        baseline_hop3  \\\n0           WinDbg was created by  Who is the current CEO of Microsoft   \n1           WinDbg was created by  Who is the current CEO of Microsoft   \n2           WinDbg was created by  Who is the current CEO of Microsoft   \n3           WinDbg was created by  Who is the current CEO of Microsoft   \n4           WinDbg was created by  Who is the current CEO of Microsoft   \n...                           ...                                  ...   \n46075  Project Ara was created by     Who is the current CEO of Google   \n46076  Project Ara was created by     Who is the current CEO of Google   \n46077  Project Ara was created by     Who is the current CEO of Google   \n46078  Project Ara was created by     Who is the current CEO of Google   \n46079  Project Ara was created by     Who is the current CEO of Google   \n\n                                      baseline_multihop3         hop1  \\\n0      Who is the current CEO of the company that cre...       WinDbg   \n1      Who is the current CEO of the company that cre...       WinDbg   \n2      Who is the current CEO of the company that cre...       WinDbg   \n3      Who is the current CEO of the company that cre...       WinDbg   \n4      Who is the current CEO of the company that cre...       WinDbg   \n...                                                  ...          ...   \n46075  Who is the current CEO of the company that cre...  Project Ara   \n46076  Who is the current CEO of the company that cre...  Project Ara   \n46077  Who is the current CEO of the company that cre...  Project Ara   \n46078  Who is the current CEO of the company that cre...  Project Ara   \n46079  Who is the current CEO of the company that cre...  Project Ara   \n\n            hop2  ... is_correct_baseline_hop3  \\\n0      Microsoft  ...                     True   \n1      Microsoft  ...                     True   \n2      Microsoft  ...                     True   \n3      Microsoft  ...                     True   \n4      Microsoft  ...                     True   \n...          ...  ...                      ...   \n46075     Google  ...                     True   \n46076     Google  ...                     True   \n46077     Google  ...                     True   \n46078     Google  ...                     True   \n46079     Google  ...                     True   \n\n                          generations_baseline_multihop3  \\\n0      ? \\n \\n I ' m trying to find out who the curre...   \n1      ? \\n \\n I ' m trying to find out who the curre...   \n2      ? \\n \\n I ' m trying to find out who the curre...   \n3      ? \\n \\n I ' m trying to find out who the curre...   \n4      ? \\n \\n I ' m trying to find out who the curre...   \n...                                                  ...   \n46075  ? \\n The current CE O of Google X , the divisi...   \n46076  ? \\n The current CE O of Google X , the divisi...   \n46077  ? \\n The current CE O of Google X , the divisi...   \n46078  ? \\n The current CE O of Google X , the divisi...   \n46079  ? \\n The current CE O of Google X , the divisi...   \n\n       is_correct_baseline_multihop3 layer_sources  layer_targets  \\\n0                              False           [0]            [0]   \n1                              False           [1]            [0]   \n2                              False           [2]            [0]   \n3                              False           [3]            [0]   \n4                              False           [4]            [0]   \n...                              ...           ...            ...   \n46075                          False          [27]           [31]   \n46076                          False          [28]           [31]   \n46077                          False          [29]           [31]   \n46078                          False          [30]           [31]   \n46079                          False          [31]           [31]   \n\n      layer_source  layer_target  \\\n0              [0]           [0]   \n1              [1]           [0]   \n2              [2]           [0]   \n3              [3]           [0]   \n4              [4]           [0]   \n...            ...           ...   \n46075         [27]          [31]   \n46076         [28]          [31]   \n46077         [29]          [31]   \n46078         [30]          [31]   \n46079         [31]          [31]   \n\n                                     generations_patched is_correct_patched  \\\n0           [d, ?, \\n, Who, is, the, current, CE, O, of]              False   \n1        [d, entity, ?, \\n, The, current, CE, O, of, by]              False   \n2         [ond, ?, \\n, The, current, CE, O, of, By, ond]              False   \n3                     [T, es, la, ?, \\n, As, of, , 2, 0]              False   \n4      [the, company, ?, \\n, \\n, Answer, :, The, curr...              False   \n...                                                  ...                ...   \n46075        [Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]              False   \n46076        [Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]              False   \n46077        [Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]              False   \n46078        [Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]              False   \n46079        [Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]              False   \n\n                                              hidden_rep  \n0      [0.01138, -0.03235, -0.00457, -0.0725, -0.0424...  \n1      [0.0697, -0.003986, -0.03278, -0.0439, -0.0864...  \n2      [0.07526, 0.01602, 0.05847, -0.05325, -0.1605,...  \n3      [0.1962, -0.1731, -0.02264, -0.1337, -0.12317,...  \n4      [0.1389, -0.2046, 0.1125, -0.1465, -0.1215, 0....  \n...                                                  ...  \n46075  [-1.918, -2.703, 3.506, -0.1589, -0.31, 1.398,...  \n46076  [-2.164, -3.111, 4.027, 0.1217, 0.5205, 1.109,...  \n46077  [-1.802, -2.348, 4.45, -0.3994, -0.503, 1.393,...  \n46078  [-1.495, -2.748, 3.39, -1.929, -0.823, 2.186, ...  \n46079  [-1.12, -1.074, 1.005, -0.03586, -0.374, 1.774...  \n\n[46080 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>prompt_source</th>\n      <th>position_source</th>\n      <th>prompt_target</th>\n      <th>position_target</th>\n      <th>baseline_hop2</th>\n      <th>baseline_hop3</th>\n      <th>baseline_multihop3</th>\n      <th>hop1</th>\n      <th>hop2</th>\n      <th>...</th>\n      <th>is_correct_baseline_hop3</th>\n      <th>generations_baseline_multihop3</th>\n      <th>is_correct_baseline_multihop3</th>\n      <th>layer_sources</th>\n      <th>layer_targets</th>\n      <th>layer_source</th>\n      <th>layer_target</th>\n      <th>generations_patched</th>\n      <th>is_correct_patched</th>\n      <th>hidden_rep</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>WinDbg was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WinDbg was created by</td>\n      <td>Who is the current CEO of Microsoft</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WinDbg</td>\n      <td>Microsoft</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n \\n I ' m trying to find out who the curre...</td>\n      <td>False</td>\n      <td>[0]</td>\n      <td>[0]</td>\n      <td>[0]</td>\n      <td>[0]</td>\n      <td>[d, ?, \\n, Who, is, the, current, CE, O, of]</td>\n      <td>False</td>\n      <td>[0.01138, -0.03235, -0.00457, -0.0725, -0.0424...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>WinDbg was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WinDbg was created by</td>\n      <td>Who is the current CEO of Microsoft</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WinDbg</td>\n      <td>Microsoft</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n \\n I ' m trying to find out who the curre...</td>\n      <td>False</td>\n      <td>[1]</td>\n      <td>[0]</td>\n      <td>[1]</td>\n      <td>[0]</td>\n      <td>[d, entity, ?, \\n, The, current, CE, O, of, by]</td>\n      <td>False</td>\n      <td>[0.0697, -0.003986, -0.03278, -0.0439, -0.0864...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>WinDbg was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WinDbg was created by</td>\n      <td>Who is the current CEO of Microsoft</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WinDbg</td>\n      <td>Microsoft</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n \\n I ' m trying to find out who the curre...</td>\n      <td>False</td>\n      <td>[2]</td>\n      <td>[0]</td>\n      <td>[2]</td>\n      <td>[0]</td>\n      <td>[ond, ?, \\n, The, current, CE, O, of, By, ond]</td>\n      <td>False</td>\n      <td>[0.07526, 0.01602, 0.05847, -0.05325, -0.1605,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>WinDbg was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WinDbg was created by</td>\n      <td>Who is the current CEO of Microsoft</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WinDbg</td>\n      <td>Microsoft</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n \\n I ' m trying to find out who the curre...</td>\n      <td>False</td>\n      <td>[3]</td>\n      <td>[0]</td>\n      <td>[3]</td>\n      <td>[0]</td>\n      <td>[T, es, la, ?, \\n, As, of, , 2, 0]</td>\n      <td>False</td>\n      <td>[0.1962, -0.1731, -0.02264, -0.1337, -0.12317,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>WinDbg was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WinDbg was created by</td>\n      <td>Who is the current CEO of Microsoft</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WinDbg</td>\n      <td>Microsoft</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n \\n I ' m trying to find out who the curre...</td>\n      <td>False</td>\n      <td>[4]</td>\n      <td>[0]</td>\n      <td>[4]</td>\n      <td>[0]</td>\n      <td>[the, company, ?, \\n, \\n, Answer, :, The, curr...</td>\n      <td>False</td>\n      <td>[0.1389, -0.2046, 0.1125, -0.1465, -0.1215, 0....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>46075</th>\n      <td>44</td>\n      <td>Project Ara was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>Project Ara was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>Project Ara</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n The current CE O of Google X , the divisi...</td>\n      <td>False</td>\n      <td>[27]</td>\n      <td>[31]</td>\n      <td>[27]</td>\n      <td>[31]</td>\n      <td>[Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]</td>\n      <td>False</td>\n      <td>[-1.918, -2.703, 3.506, -0.1589, -0.31, 1.398,...</td>\n    </tr>\n    <tr>\n      <th>46076</th>\n      <td>44</td>\n      <td>Project Ara was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>Project Ara was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>Project Ara</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n The current CE O of Google X , the divisi...</td>\n      <td>False</td>\n      <td>[28]</td>\n      <td>[31]</td>\n      <td>[28]</td>\n      <td>[31]</td>\n      <td>[Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]</td>\n      <td>False</td>\n      <td>[-2.164, -3.111, 4.027, 0.1217, 0.5205, 1.109,...</td>\n    </tr>\n    <tr>\n      <th>46077</th>\n      <td>44</td>\n      <td>Project Ara was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>Project Ara was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>Project Ara</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n The current CE O of Google X , the divisi...</td>\n      <td>False</td>\n      <td>[29]</td>\n      <td>[31]</td>\n      <td>[29]</td>\n      <td>[31]</td>\n      <td>[Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]</td>\n      <td>False</td>\n      <td>[-1.802, -2.348, 4.45, -0.3994, -0.503, 1.393,...</td>\n    </tr>\n    <tr>\n      <th>46078</th>\n      <td>44</td>\n      <td>Project Ara was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>Project Ara was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>Project Ara</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n The current CE O of Google X , the divisi...</td>\n      <td>False</td>\n      <td>[30]</td>\n      <td>[31]</td>\n      <td>[30]</td>\n      <td>[31]</td>\n      <td>[Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]</td>\n      <td>False</td>\n      <td>[-1.495, -2.748, 3.39, -1.929, -0.823, 2.186, ...</td>\n    </tr>\n    <tr>\n      <th>46079</th>\n      <td>44</td>\n      <td>Project Ara was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>Project Ara was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>Project Ara</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>True</td>\n      <td>? \\n The current CE O of Google X , the divisi...</td>\n      <td>False</td>\n      <td>[31]</td>\n      <td>[31]</td>\n      <td>[31]</td>\n      <td>[31]</td>\n      <td>[Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]</td>\n      <td>False</td>\n      <td>[-1.12, -1.074, 1.005, -0.03586, -0.374, 1.774...</td>\n    </tr>\n  </tbody>\n</table>\n<p>46080 rows  24 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(\"./outputs/factual/multihop_product_company_ceo.pkl\",\n",
    "               \"./outputs/results/factual\",\n",
    "               fname_out=\"multihop_product_company_ceo\", batch_size=128 // batch_size_scale, n_samples=-1,\n",
    "               save_output=True, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUhtDO_Uap1K"
   },
   "source": [
    "# Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LGCuzYy7ap1K",
    "ExecuteTime": {
     "end_time": "2024-06-26T17:18:17.186914Z",
     "start_time": "2024-06-26T17:18:17.176733400Z"
    }
   },
   "outputs": [],
   "source": [
    "def probe_baseline(task_type=\"factual\", task_name=\"multihop_product_company_ceo\",\n",
    "                   fname_input=\"./outputs/factual/multihop_product_company_ceo.pkl\",\n",
    "                   inp_label_name=\"object\",\n",
    "                   hidden_states_dir=\"./outputs/results_ceo\",\n",
    "                   probe_res_dir=\"./outputs/probe_ceo\",\n",
    "                   label_name=\"hop3\", seed=42, n_test_samples=2,  # test_ratio=0.5, n_samples=4,\n",
    "                   rewrite=False, only_correct=True):\n",
    "    fdir = f\"{probe_res_dir}/{task_type}\"\n",
    "    fname_pkl = f\"{fdir}/{task_name}_only_correct_{only_correct}.pkl\"\n",
    "    if rewrite == False and os.path.exists(fname_pkl):\n",
    "        print(f\"\\t{fname_pkl} exists. Skipping generation. Reading file...\")\n",
    "        test_df = pd.read_pickle(fname_pkl)\n",
    "        return test_df\n",
    "    print(f\"Creating {fname_pkl}...\")\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    fname_hidden_states = f\"{hidden_states_dir}/{task_type}/{task_name}.pkl\"\n",
    "\n",
    "    # Retrieve list of classes from inputs\n",
    "    inps_df = pd.read_csv(fname_input, sep='\\t', header=0)\n",
    "    classes = np.unique(inps_df[inp_label_name])\n",
    "    classes_dict = {}\n",
    "    for idx, cls in enumerate(classes):\n",
    "        classes_dict[cls] = idx\n",
    "\n",
    "    # Get saved hiddens\n",
    "    hiddens_df = pd.read_pickle(fname_hidden_states)\n",
    "    hiddens_df = hiddens_df.sample(frac=1).reset_index(drop=True)\n",
    "    if only_correct:\n",
    "        hiddens_df = hiddens_df[hiddens_df[\"is_correct_baseline_hop2\"]].reset_index(drop=True)\n",
    "        hiddens_df = hiddens_df[hiddens_df[\"is_correct_baseline_hop3\"]].reset_index(drop=True)\n",
    "        if len(hiddens_df) < 1:\n",
    "            print(f'\\tNo correct predictions for {fname_pkl}. Skipping...')\n",
    "            return\n",
    "    sample_ids = np.unique(hiddens_df['sample_id'])\n",
    "    if len(sample_ids) < 4:\n",
    "        print(f\"\\tNot enough samples to train a probe for {fname_pkl}. Skipping...\")\n",
    "        return\n",
    "    np.random.shuffle(sample_ids)\n",
    "    test_sample_ids = sample_ids[:n_test_samples]\n",
    "    train_sample_ids = sample_ids[n_test_samples:]\n",
    "    train_df = hiddens_df[hiddens_df['sample_id'].isin(train_sample_ids)]\n",
    "    test_df = hiddens_df[hiddens_df['sample_id'].isin(test_sample_ids)]\n",
    "    xs = np.stack(hiddens_df[\"hidden_rep\"])\n",
    "    ys = np.array([classes_dict[i] for i in hiddens_df[label_name]])\n",
    "\n",
    "    train_xs = np.stack(train_df[\"hidden_rep\"])\n",
    "    train_ys = np.array([classes_dict[i] for i in train_df[label_name]])\n",
    "    if len(np.unique(train_ys)) < 2:\n",
    "        print(f\"\\tNot enough variety to train a probe for {fname_pkl}. Skipping...\")\n",
    "        return\n",
    "    test_xs = np.stack(test_df[\"hidden_rep\"])\n",
    "    test_ys = np.array([classes_dict[i] for i in test_df[label_name]])\n",
    "\n",
    "    clf = LogisticRegression(random_state=seed).fit(train_xs, train_ys)\n",
    "    predicted_ys = clf.predict(test_xs)\n",
    "    test_df[\"object_int\"] = test_ys\n",
    "    test_df[\"predicted_int\"] = predicted_ys\n",
    "    test_df[\"predicted\"] = classes[predicted_ys]\n",
    "    test_df[\"is_correct_probe\"] = test_ys == predicted_ys\n",
    "\n",
    "    if not os.path.exists(fdir):\n",
    "        os.makedirs(fdir)\n",
    "    test_df.to_csv(os.path.join(fdir, f\"{task_name}_only_correct_{only_correct}.tsv\"), sep=\"\\t\")\n",
    "    test_df.to_pickle(os.path.join(fdir, f\"{task_name}_only_correct_{only_correct}.pkl\"))\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4LMi-JxZap1K",
    "ExecuteTime": {
     "end_time": "2024-06-26T17:18:17.253707100Z",
     "start_time": "2024-06-26T17:18:17.186914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t./outputs/probe_ceo/factual/multihop_product_company_ceo_only_correct_True.pkl exists. Skipping generation. Reading file...\n"
     ]
    },
    {
     "data": {
      "text/plain": "       sample_id                 prompt_source  position_source  \\\n24            35  WebP Lossless was created by               -1   \n31            29    Macbook Pro was created by               -1   \n42            35  WebP Lossless was created by               -1   \n52            35  WebP Lossless was created by               -1   \n55            35  WebP Lossless was created by               -1   \n...          ...                           ...              ...   \n31625         29    Macbook Pro was created by               -1   \n31643         35  WebP Lossless was created by               -1   \n31647         29    Macbook Pro was created by               -1   \n31663         35  WebP Lossless was created by               -1   \n31684         35  WebP Lossless was created by               -1   \n\n                      prompt_target  position_target  \\\n24     Who is the current CEO of {}               -1   \n31     Who is the current CEO of {}               -1   \n42     Who is the current CEO of {}               -1   \n52     Who is the current CEO of {}               -1   \n55     Who is the current CEO of {}               -1   \n...                             ...              ...   \n31625  Who is the current CEO of {}               -1   \n31643  Who is the current CEO of {}               -1   \n31647  Who is the current CEO of {}               -1   \n31663  Who is the current CEO of {}               -1   \n31684  Who is the current CEO of {}               -1   \n\n                      baseline_hop2                     baseline_hop3  \\\n24     WebP Lossless was created by  Who is the current CEO of Google   \n31       Macbook Pro was created by   Who is the current CEO of Apple   \n42     WebP Lossless was created by  Who is the current CEO of Google   \n52     WebP Lossless was created by  Who is the current CEO of Google   \n55     WebP Lossless was created by  Who is the current CEO of Google   \n...                             ...                               ...   \n31625    Macbook Pro was created by   Who is the current CEO of Apple   \n31643  WebP Lossless was created by  Who is the current CEO of Google   \n31647    Macbook Pro was created by   Who is the current CEO of Apple   \n31663  WebP Lossless was created by  Who is the current CEO of Google   \n31684  WebP Lossless was created by  Who is the current CEO of Google   \n\n                                      baseline_multihop3           hop1  \\\n24     Who is the current CEO of the company that cre...  WebP Lossless   \n31     Who is the current CEO of the company that cre...    Macbook Pro   \n42     Who is the current CEO of the company that cre...  WebP Lossless   \n52     Who is the current CEO of the company that cre...  WebP Lossless   \n55     Who is the current CEO of the company that cre...  WebP Lossless   \n...                                                  ...            ...   \n31625  Who is the current CEO of the company that cre...    Macbook Pro   \n31643  Who is the current CEO of the company that cre...  WebP Lossless   \n31647  Who is the current CEO of the company that cre...    Macbook Pro   \n31663  Who is the current CEO of the company that cre...  WebP Lossless   \n31684  Who is the current CEO of the company that cre...  WebP Lossless   \n\n         hop2  ... is_correct_baseline_multihop3 layer_source  layer_target  \\\n24     Google  ...                         False           20             4   \n31      Apple  ...                         False           11             9   \n42     Google  ...                         False            8            24   \n52     Google  ...                         False           12            13   \n55     Google  ...                         False           21            23   \n...       ...  ...                           ...          ...           ...   \n31625   Apple  ...                         False            5            17   \n31643  Google  ...                         False           19            11   \n31647   Apple  ...                         False            4            13   \n31663  Google  ...                         False           13             8   \n31684  Google  ...                         False            8            14   \n\n                                     generations_patched  is_correct_patched  \\\n24     [Google, ?, \\n, The, current, CE, O, of, Googl...               False   \n31      [the, company, ?, \\n, \\n, a, ., John, Smith, \\n]               False   \n42      [the, company, ?, \\n, \\n, a, ., John, Smith, \\n]               False   \n52      [the, company, ?, \\n, \\n, a, ., John, Smith, \\n]               False   \n55           [Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]               False   \n...                                                  ...                 ...   \n31625   [the, company, ?, \\n, \\n, a, ., John, Smith, \\n]               False   \n31643        [Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]               False   \n31647   [the, company, ?, \\n, \\n, a, ., John, Smith, \\n]               False   \n31663   [the, company, ?, \\n, \\n, a, ., John, Smith, \\n]               False   \n31684   [the, company, ?, \\n, \\n, a, ., John, Smith, \\n]               False   \n\n                                              hidden_rep  object_int  \\\n24     [0.9727, -0.6094, 0.1852, 0.5312, -0.4336, 1.7...         173   \n31     [0.1063, -0.537, 0.1333, -0.148, -0.2118, 0.05...         181   \n42     [0.1389, -0.468, 0.002441, -0.3086, 0.125, 0.0...         173   \n52     [0.4866, -0.676, 0.2292, -0.1644, -0.2769, -0....         173   \n55     [0.658, -0.8945, 0.601, 0.9395, -0.898, 1.819,...         173   \n...                                                  ...         ...   \n31625  [0.1724, -0.1149, 0.1431, -0.08514, -0.093, 0....         181   \n31643  [0.4497, -0.2876, 0.0279, 0.4854, -0.3582, 1.4...         173   \n31647  [0.1017, -0.1692, 0.128, -0.0725, -0.0798, 0.1...         181   \n31663  [0.585, -0.362, 0.106, 0.004395, -0.592, 0.158...         173   \n31684  [0.1389, -0.468, 0.002441, -0.3086, 0.125, 0.0...         173   \n\n       predicted_int      predicted is_correct_probe  \n24               173  Sundar Pichai             True  \n31               181       Tim Cook             True  \n42               173  Sundar Pichai             True  \n52               173  Sundar Pichai             True  \n55               173  Sundar Pichai             True  \n...              ...            ...              ...  \n31625            181       Tim Cook             True  \n31643            173  Sundar Pichai             True  \n31647            181       Tim Cook             True  \n31663            173  Sundar Pichai             True  \n31684            173  Sundar Pichai             True  \n\n[2048 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>prompt_source</th>\n      <th>position_source</th>\n      <th>prompt_target</th>\n      <th>position_target</th>\n      <th>baseline_hop2</th>\n      <th>baseline_hop3</th>\n      <th>baseline_multihop3</th>\n      <th>hop1</th>\n      <th>hop2</th>\n      <th>...</th>\n      <th>is_correct_baseline_multihop3</th>\n      <th>layer_source</th>\n      <th>layer_target</th>\n      <th>generations_patched</th>\n      <th>is_correct_patched</th>\n      <th>hidden_rep</th>\n      <th>object_int</th>\n      <th>predicted_int</th>\n      <th>predicted</th>\n      <th>is_correct_probe</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24</th>\n      <td>35</td>\n      <td>WebP Lossless was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WebP Lossless was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WebP Lossless</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>False</td>\n      <td>20</td>\n      <td>4</td>\n      <td>[Google, ?, \\n, The, current, CE, O, of, Googl...</td>\n      <td>False</td>\n      <td>[0.9727, -0.6094, 0.1852, 0.5312, -0.4336, 1.7...</td>\n      <td>173</td>\n      <td>173</td>\n      <td>Sundar Pichai</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>29</td>\n      <td>Macbook Pro was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>Macbook Pro was created by</td>\n      <td>Who is the current CEO of Apple</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>Macbook Pro</td>\n      <td>Apple</td>\n      <td>...</td>\n      <td>False</td>\n      <td>11</td>\n      <td>9</td>\n      <td>[the, company, ?, \\n, \\n, a, ., John, Smith, \\n]</td>\n      <td>False</td>\n      <td>[0.1063, -0.537, 0.1333, -0.148, -0.2118, 0.05...</td>\n      <td>181</td>\n      <td>181</td>\n      <td>Tim Cook</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>35</td>\n      <td>WebP Lossless was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WebP Lossless was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WebP Lossless</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>False</td>\n      <td>8</td>\n      <td>24</td>\n      <td>[the, company, ?, \\n, \\n, a, ., John, Smith, \\n]</td>\n      <td>False</td>\n      <td>[0.1389, -0.468, 0.002441, -0.3086, 0.125, 0.0...</td>\n      <td>173</td>\n      <td>173</td>\n      <td>Sundar Pichai</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>35</td>\n      <td>WebP Lossless was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WebP Lossless was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WebP Lossless</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>False</td>\n      <td>12</td>\n      <td>13</td>\n      <td>[the, company, ?, \\n, \\n, a, ., John, Smith, \\n]</td>\n      <td>False</td>\n      <td>[0.4866, -0.676, 0.2292, -0.1644, -0.2769, -0....</td>\n      <td>173</td>\n      <td>173</td>\n      <td>Sundar Pichai</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>35</td>\n      <td>WebP Lossless was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WebP Lossless was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WebP Lossless</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>False</td>\n      <td>21</td>\n      <td>23</td>\n      <td>[Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]</td>\n      <td>False</td>\n      <td>[0.658, -0.8945, 0.601, 0.9395, -0.898, 1.819,...</td>\n      <td>173</td>\n      <td>173</td>\n      <td>Sundar Pichai</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31625</th>\n      <td>29</td>\n      <td>Macbook Pro was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>Macbook Pro was created by</td>\n      <td>Who is the current CEO of Apple</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>Macbook Pro</td>\n      <td>Apple</td>\n      <td>...</td>\n      <td>False</td>\n      <td>5</td>\n      <td>17</td>\n      <td>[the, company, ?, \\n, \\n, a, ., John, Smith, \\n]</td>\n      <td>False</td>\n      <td>[0.1724, -0.1149, 0.1431, -0.08514, -0.093, 0....</td>\n      <td>181</td>\n      <td>181</td>\n      <td>Tim Cook</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>31643</th>\n      <td>35</td>\n      <td>WebP Lossless was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WebP Lossless was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WebP Lossless</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>False</td>\n      <td>19</td>\n      <td>11</td>\n      <td>[Google, ?, \\n, \\n, a, ., Sund, ar, P, ich]</td>\n      <td>False</td>\n      <td>[0.4497, -0.2876, 0.0279, 0.4854, -0.3582, 1.4...</td>\n      <td>173</td>\n      <td>173</td>\n      <td>Sundar Pichai</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>31647</th>\n      <td>29</td>\n      <td>Macbook Pro was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>Macbook Pro was created by</td>\n      <td>Who is the current CEO of Apple</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>Macbook Pro</td>\n      <td>Apple</td>\n      <td>...</td>\n      <td>False</td>\n      <td>4</td>\n      <td>13</td>\n      <td>[the, company, ?, \\n, \\n, a, ., John, Smith, \\n]</td>\n      <td>False</td>\n      <td>[0.1017, -0.1692, 0.128, -0.0725, -0.0798, 0.1...</td>\n      <td>181</td>\n      <td>181</td>\n      <td>Tim Cook</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>31663</th>\n      <td>35</td>\n      <td>WebP Lossless was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WebP Lossless was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WebP Lossless</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>False</td>\n      <td>13</td>\n      <td>8</td>\n      <td>[the, company, ?, \\n, \\n, a, ., John, Smith, \\n]</td>\n      <td>False</td>\n      <td>[0.585, -0.362, 0.106, 0.004395, -0.592, 0.158...</td>\n      <td>173</td>\n      <td>173</td>\n      <td>Sundar Pichai</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>31684</th>\n      <td>35</td>\n      <td>WebP Lossless was created by</td>\n      <td>-1</td>\n      <td>Who is the current CEO of {}</td>\n      <td>-1</td>\n      <td>WebP Lossless was created by</td>\n      <td>Who is the current CEO of Google</td>\n      <td>Who is the current CEO of the company that cre...</td>\n      <td>WebP Lossless</td>\n      <td>Google</td>\n      <td>...</td>\n      <td>False</td>\n      <td>8</td>\n      <td>14</td>\n      <td>[the, company, ?, \\n, \\n, a, ., John, Smith, \\n]</td>\n      <td>False</td>\n      <td>[0.1389, -0.468, 0.002441, -0.3086, 0.125, 0.0...</td>\n      <td>173</td>\n      <td>173</td>\n      <td>Sundar Pichai</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>2048 rows  26 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe_baseline(task_type=\"factual\", task_name=\"multihop_product_company_ceo\",\n",
    "               # fname_input=\"./preprocessed_data/factual/company_ceo.pkl\",\n",
    "               fname_input=\"./preprocessed_data/factual/company_ceo.tsv\",\n",
    "               inp_label_name=\"object\",\n",
    "               hidden_states_dir=\"./outputs/results\",\n",
    "               probe_res_dir=\"./outputs/probe_ceo\",\n",
    "               label_name=\"hop3\",\n",
    "               rewrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgj0nbODap1K"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "U-WLsB0Map1K",
    "ExecuteTime": {
     "end_time": "2024-06-26T17:18:17.256713400Z",
     "start_time": "2024-06-26T17:18:17.234689200Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_heatmaps(task_type=\"factual\", task_name=\"multihop_product_company_ceo\", version=\"v5\", _vmin=0, _vmax=1):\n",
    "    probe_res_fname = f\"./outputs/probe_{version}/{task_type}/{task_name}_only_correct_True.pkl\"\n",
    "    probe_df = pd.read_pickle(probe_res_fname)\n",
    "    plot_ttl = f\"{task_type} : {task_name} - {model_name.strip('./')}\"\n",
    "    n_samples = len(probe_df)\n",
    "\n",
    "    heatmap_patch = probe_df.groupby(['layer_target', 'layer_source'])[\"is_correct_patched\"].mean().unstack()\n",
    "    ax = sns.heatmap(data=heatmap_patch, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"{plot_ttl} \\npatch accuracy (# samples: {n_samples})\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    ax = sns.lineplot(data=probe_df, x=\"layer_source\", y=\"is_correct_probe\")\n",
    "    ax.set_ylim(-0.01, 1.01)\n",
    "    ax.set_title(f\"{plot_ttl} \\nprobe accuracy (# samples: {n_samples})\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "q2iCyXYGap1K",
    "ExecuteTime": {
     "end_time": "2024-06-26T17:18:17.280092500Z",
     "start_time": "2024-06-26T17:18:17.249707300Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_patching_heatmaps(task_type=\"factual\", task_name=\"multihop_product_company_ceo\", version=\"ceo\",\n",
    "                           _vmin=0, _vmax=1):\n",
    "    patch_res_fname = f\"./outputs/results/{task_type}/{task_name}.pkl\"\n",
    "    patch_df = pd.read_pickle(patch_res_fname)\n",
    "    patch_df = patch_df[patch_df[\"is_correct_baseline_hop2\"]].reset_index(drop=True)\n",
    "    patch_df = patch_df[patch_df[\"is_correct_baseline_hop3\"]].reset_index(drop=True)\n",
    "    n_samples = len(patch_df)\n",
    "    if n_samples == 0:\n",
    "        print(f\"No correct predictions for {patch_res_fname}. Skipping...\")\n",
    "        return\n",
    "    plot_ttl = f\"{task_type}: {task_name}\\n{model_name.strip('./')}\"\n",
    "    baseline_acc_multihop3 = patch_df[\"is_correct_baseline_multihop3\"].mean() * 100\n",
    "    baseline_acc_hop3 = patch_df[\"is_correct_baseline_hop3\"].mean() * 100\n",
    "    baseline_acc_hop2 = patch_df[\"is_correct_baseline_hop2\"].mean() * 100\n",
    "\n",
    "    heatmap_patched = patch_df.groupby(['layer_target', 'layer_source'])[\"is_correct_patched\"].mean().unstack()\n",
    "    ax = sns.heatmap(data=heatmap_patched, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(\n",
    "        f\"{plot_ttl}\\nPatching accuracy\\nBaseline multihop reasoning accuracy: {baseline_acc_multihop3:.2f}\\n(# samples: {n_samples})\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3SIl8dtZap1K",
    "outputId": "5213d942-f46a-444a-9a7e-ca1004169398",
    "ExecuteTime": {
     "end_time": "2024-06-26T17:18:18.362910400Z",
     "start_time": "2024-06-26T17:18:17.262716300Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mplot_patching_heatmaps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mceo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[20], line 16\u001B[0m, in \u001B[0;36mplot_patching_heatmaps\u001B[1;34m(task_type, task_name, version, _vmin, _vmax)\u001B[0m\n\u001B[0;32m     13\u001B[0m baseline_acc_hop3 \u001B[38;5;241m=\u001B[39m patch_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_correct_baseline_hop3\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmean() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[0;32m     14\u001B[0m baseline_acc_hop2 \u001B[38;5;241m=\u001B[39m patch_df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_correct_baseline_hop2\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmean() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m\n\u001B[1;32m---> 16\u001B[0m heatmap_patched \u001B[38;5;241m=\u001B[39m \u001B[43mpatch_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlayer_target\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlayer_source\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mis_correct_patched\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39munstack()\n\u001B[0;32m     17\u001B[0m ax \u001B[38;5;241m=\u001B[39m sns\u001B[38;5;241m.\u001B[39mheatmap(data\u001B[38;5;241m=\u001B[39mheatmap_patched, cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcrest_r\u001B[39m\u001B[38;5;124m\"\u001B[39m, vmin\u001B[38;5;241m=\u001B[39m_vmin, vmax\u001B[38;5;241m=\u001B[39m_vmax)\n\u001B[0;32m     18\u001B[0m ax\u001B[38;5;241m.\u001B[39minvert_yaxis()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:2452\u001B[0m, in \u001B[0;36mGroupBy.mean\u001B[1;34m(self, numeric_only, engine, engine_kwargs)\u001B[0m\n\u001B[0;32m   2445\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_numba_agg_general(\n\u001B[0;32m   2446\u001B[0m         grouped_mean,\n\u001B[0;32m   2447\u001B[0m         executor\u001B[38;5;241m.\u001B[39mfloat_dtype_mapping,\n\u001B[0;32m   2448\u001B[0m         engine_kwargs,\n\u001B[0;32m   2449\u001B[0m         min_periods\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   2450\u001B[0m     )\n\u001B[0;32m   2451\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2452\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cython_agg_general\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2453\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmean\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2454\u001B[0m \u001B[43m        \u001B[49m\u001B[43malt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnumeric_only\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2455\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnumeric_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnumeric_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2456\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgroupby\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1998\u001B[0m, in \u001B[0;36mGroupBy._cython_agg_general\u001B[1;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001B[0m\n\u001B[0;32m   1995\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_agg_py_fallback(how, values, ndim\u001B[38;5;241m=\u001B[39mdata\u001B[38;5;241m.\u001B[39mndim, alt\u001B[38;5;241m=\u001B[39malt)\n\u001B[0;32m   1996\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[1;32m-> 1998\u001B[0m new_mgr \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrouped_reduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray_func\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1999\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_agged_manager(new_mgr)\n\u001B[0;32m   2000\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124midxmin\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124midxmax\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\internals\\base.py:367\u001B[0m, in \u001B[0;36mSingleDataManager.grouped_reduce\u001B[1;34m(self, func)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgrouped_reduce\u001B[39m(\u001B[38;5;28mself\u001B[39m, func):\n\u001B[0;32m    366\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39marray\n\u001B[1;32m--> 367\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    368\u001B[0m     index \u001B[38;5;241m=\u001B[39m default_index(\u001B[38;5;28mlen\u001B[39m(res))\n\u001B[0;32m    370\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mfrom_array(res, index)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1973\u001B[0m, in \u001B[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001B[1;34m(values)\u001B[0m\n\u001B[0;32m   1971\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21marray_func\u001B[39m(values: ArrayLike) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ArrayLike:\n\u001B[0;32m   1972\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1973\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_grouper\u001B[38;5;241m.\u001B[39m_cython_operation(\n\u001B[0;32m   1974\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maggregate\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1975\u001B[0m             values,\n\u001B[0;32m   1976\u001B[0m             how,\n\u001B[0;32m   1977\u001B[0m             axis\u001B[38;5;241m=\u001B[39mdata\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1978\u001B[0m             min_count\u001B[38;5;241m=\u001B[39mmin_count,\n\u001B[0;32m   1979\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   1980\u001B[0m         )\n\u001B[0;32m   1981\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m:\n\u001B[0;32m   1982\u001B[0m         \u001B[38;5;66;03m# generally if we have numeric_only=False\u001B[39;00m\n\u001B[0;32m   1983\u001B[0m         \u001B[38;5;66;03m# and non-applicable functions\u001B[39;00m\n\u001B[0;32m   1984\u001B[0m         \u001B[38;5;66;03m# try to python agg\u001B[39;00m\n\u001B[0;32m   1985\u001B[0m         \u001B[38;5;66;03m# TODO: shouldn't min_count matter?\u001B[39;00m\n\u001B[0;32m   1986\u001B[0m         \u001B[38;5;66;03m# TODO: avoid special casing SparseArray here\u001B[39;00m\n\u001B[0;32m   1987\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(values, SparseArray):\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:827\u001B[0m, in \u001B[0;36mBaseGrouper._cython_operation\u001B[1;34m(self, kind, values, how, axis, min_count, **kwargs)\u001B[0m\n\u001B[0;32m    822\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    823\u001B[0m \u001B[38;5;124;03mReturns the values of a cython operation.\u001B[39;00m\n\u001B[0;32m    824\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    825\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m kind \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransform\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maggregate\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m--> 827\u001B[0m cy_op \u001B[38;5;241m=\u001B[39m WrappedCythonOp(kind\u001B[38;5;241m=\u001B[39mkind, how\u001B[38;5;241m=\u001B[39mhow, has_dropped_na\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_dropped_na\u001B[49m)\n\u001B[0;32m    829\u001B[0m ids, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroup_info\n\u001B[0;32m    830\u001B[0m ngroups \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mngroups\n",
      "File \u001B[1;32mproperties.pyx:36\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:741\u001B[0m, in \u001B[0;36mBaseGrouper.has_dropped_na\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhas_dropped_na\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m    738\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    739\u001B[0m \u001B[38;5;124;03m    Whether grouper has null value(s) that are dropped.\u001B[39;00m\n\u001B[0;32m    740\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 741\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m((\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroup_info\u001B[49m[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39many())\n",
      "File \u001B[1;32mproperties.pyx:36\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:745\u001B[0m, in \u001B[0;36mBaseGrouper.group_info\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    743\u001B[0m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgroup_info\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39mintp], npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39mintp], \u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m--> 745\u001B[0m     comp_ids, obs_group_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_compressed_codes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    747\u001B[0m     ngroups \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(obs_group_ids)\n\u001B[0;32m    748\u001B[0m     comp_ids \u001B[38;5;241m=\u001B[39m ensure_platform_int(comp_ids)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:764\u001B[0m, in \u001B[0;36mBaseGrouper._get_compressed_codes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    758\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m    759\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_compressed_codes\u001B[39m(\n\u001B[0;32m    760\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    761\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39msignedinteger], npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39mintp]]:\n\u001B[0;32m    762\u001B[0m     \u001B[38;5;66;03m# The first returned ndarray may have any signed integer dtype\u001B[39;00m\n\u001B[0;32m    763\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroupings) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 764\u001B[0m         group_index \u001B[38;5;241m=\u001B[39m get_group_index(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcodes\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape, sort\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, xnull\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    765\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m compress_group_index(group_index, sort\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sort)\n\u001B[0;32m    766\u001B[0m         \u001B[38;5;66;03m# FIXME: compress_group_index's second return value is int64, not intp\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:690\u001B[0m, in \u001B[0;36mBaseGrouper.codes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    687\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m    688\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    689\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcodes\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39msignedinteger]]:\n\u001B[1;32m--> 690\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [ping\u001B[38;5;241m.\u001B[39mcodes \u001B[38;5;28;01mfor\u001B[39;00m ping \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroupings]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:690\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    687\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m    688\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    689\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcodes\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39msignedinteger]]:\n\u001B[1;32m--> 690\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mping\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcodes\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m ping \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroupings]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:691\u001B[0m, in \u001B[0;36mGrouping.codes\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    689\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcodes\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m npt\u001B[38;5;241m.\u001B[39mNDArray[np\u001B[38;5;241m.\u001B[39msignedinteger]:\n\u001B[1;32m--> 691\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_codes_and_uniques\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mproperties.pyx:36\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:835\u001B[0m, in \u001B[0;36mGrouping._codes_and_uniques\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    830\u001B[0m     uniques \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_uniques\n\u001B[0;32m    831\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    832\u001B[0m     \u001B[38;5;66;03m# GH35667, replace dropna=False with use_na_sentinel=False\u001B[39;00m\n\u001B[0;32m    833\u001B[0m     \u001B[38;5;66;03m# error: Incompatible types in assignment (expression has type \"Union[\u001B[39;00m\n\u001B[0;32m    834\u001B[0m     \u001B[38;5;66;03m# ndarray[Any, Any], Index]\", variable has type \"Categorical\")\u001B[39;00m\n\u001B[1;32m--> 835\u001B[0m     codes, uniques \u001B[38;5;241m=\u001B[39m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfactorize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[assignment]\u001B[39;49;00m\n\u001B[0;32m    836\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrouping_vector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sort\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_na_sentinel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dropna\u001B[49m\n\u001B[0;32m    837\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    838\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m codes, uniques\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\algorithms.py:795\u001B[0m, in \u001B[0;36mfactorize\u001B[1;34m(values, sort, use_na_sentinel, size_hint)\u001B[0m\n\u001B[0;32m    792\u001B[0m             \u001B[38;5;66;03m# Don't modify (potentially user-provided) array\u001B[39;00m\n\u001B[0;32m    793\u001B[0m             values \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(null_mask, na_value, values)\n\u001B[1;32m--> 795\u001B[0m     codes, uniques \u001B[38;5;241m=\u001B[39m \u001B[43mfactorize_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_na_sentinel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_na_sentinel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m        \u001B[49m\u001B[43msize_hint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize_hint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sort \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    802\u001B[0m     uniques, codes \u001B[38;5;241m=\u001B[39m safe_sort(\n\u001B[0;32m    803\u001B[0m         uniques,\n\u001B[0;32m    804\u001B[0m         codes,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m         verify\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    808\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\patchscopes\\lib\\site-packages\\pandas\\core\\algorithms.py:595\u001B[0m, in \u001B[0;36mfactorize_array\u001B[1;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001B[0m\n\u001B[0;32m    592\u001B[0m hash_klass, values \u001B[38;5;241m=\u001B[39m _get_hashtable_algo(values)\n\u001B[0;32m    594\u001B[0m table \u001B[38;5;241m=\u001B[39m hash_klass(size_hint \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(values))\n\u001B[1;32m--> 595\u001B[0m uniques, codes \u001B[38;5;241m=\u001B[39m \u001B[43mtable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfactorize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    596\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    597\u001B[0m \u001B[43m    \u001B[49m\u001B[43mna_sentinel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    598\u001B[0m \u001B[43m    \u001B[49m\u001B[43mna_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    599\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    600\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_na\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_na_sentinel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    603\u001B[0m \u001B[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001B[39;00m\n\u001B[0;32m    604\u001B[0m uniques \u001B[38;5;241m=\u001B[39m _reconstruct_data(uniques, original\u001B[38;5;241m.\u001B[39mdtype, original)\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7281\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7195\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "plot_patching_heatmaps(version=\"ceo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwFJIvpgap1K",
    "outputId": "cb784d31-ec17-499f-e673-33347886f9b6",
    "ExecuteTime": {
     "start_time": "2024-06-26T17:18:18.359910900Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_heatmaps(version=\"ceo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghlFBvz6ap1L"
   },
   "source": [
    "# Experiment 2 : CoT experiment subset\n",
    "\n",
    "This is a subset made only from (product, company) and (company, CEO) tuples from the LRE dataset.\n",
    "We only picked 3 (company, CEO) tuples, and 15 (product, company) tuples for each that the model is more likely to know the answer to.\n",
    "\n",
    "This is an exploratory experiment. There is a more complete experiment later in the colab.\n",
    "Hop 1: Product\n",
    "Hop 2: company\n",
    "Hop 3: CEO\n",
    "\n",
    "The difference between this and experiment 1 is in the choice of source and target prompt template. In this experiment, concatenation of source and target prompt makes a reasonable query, compared to experiment 1 where they where that wasn't the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ea11E8dnap1L",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.838784Z"
    }
   },
   "outputs": [],
   "source": [
    "multihop_samples = {\n",
    "    (\"Satya Nadella\", \"Microsoft\"): [\"WinDbg\", \".NET Framework\", \"Internet Explorer\", \"MS-DOS\", \"Office Open XML\",\n",
    "                                     \"TypeScript\", \"Bing Maps Platform\", \"Outlook Express\", \"PowerShell\", \"Windows 95\",\n",
    "                                     \"Xbox 360\", \"Zune\", \"Visual Basic Script\", \"Virtual Hard Disk\", \"Robocopy\",\n",
    "                                     ],\n",
    "    (\"Tim Cook\", \"Apple\"): [\"Siri\", \"App Store\", \"CarPlay\", \"MacBook Air\", \"Xcode\",\n",
    "                            \"macOS\", \"iWork\", \"Safari\", \"QuickTime\", \"TextEdit\",\n",
    "                            \"WebKit\", \"QuickDraw\", \"Time Machine (macOS)\", \"MessagePad\", \"Macbook Pro\",\n",
    "                            ],\n",
    "    (\"Sundar Pichai\", \"Google\"): [\"Chromecast\", \"Chromebook\", \"Wear OS\", \"G Suite\", \"Picasa\",\n",
    "                                  \"WebP Lossless\", \"General Transit Feed Specification Lossless\", \"Cloud Spanner\",\n",
    "                                  \"Android TV\", \"Android Runtime\",\n",
    "                                  \"Android Jelly Bean\", \"Android Auto\", \"App Inventor\", \"Chromebook Pixel\",\n",
    "                                  \"Project Ara\",\n",
    "                                  ]\n",
    "}\n",
    "\n",
    "\n",
    "def generate_CoT_data_prod(fdir_out=\"./outputs/preprocessed_data_prod_CoT/factual\", batch_size=512 // batch_size_scale,\n",
    "                           max_gen_len=20):\n",
    "    if not os.path.exists(fdir_out):\n",
    "        os.makedirs(fdir_out)\n",
    "    prompt_source_template = \"Who is the current CEO of \"\n",
    "    prompt_target_template = \"the company that created {}\"\n",
    "    sample_id = 0\n",
    "\n",
    "    print(\"Step 1: Prepare dataset...\")\n",
    "    records = []\n",
    "\n",
    "    for key, value in multihop_samples.items():\n",
    "        hop3, hop2 = key\n",
    "        for hop1 in value:\n",
    "            # hop1: Product\n",
    "            # hop2: Company\n",
    "            # hop3: CEO\n",
    "\n",
    "            records.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"prompt_source\": prompt_source_template,\n",
    "                \"position_source\": -1,  # always doing next token prediction\n",
    "                \"prompt_target\": prompt_target_template.replace(\"{}\", hop1),\n",
    "                \"position_target\": -1,\n",
    "\n",
    "                \"baseline_hop2\": f\"the company that created {hop1}\",  #  hop2\n",
    "                \"baseline_hop3\": f\"Who is the current CEO of {hop2}\",  # hop3\n",
    "                \"baseline_multihop3\": f\"Who is the current CEO of the company that created {hop1}\",  # hop3\n",
    "\n",
    "                \"hop1\": hop1,\n",
    "                \"hop2\": hop2,\n",
    "                \"hop3\": hop3,\n",
    "            })\n",
    "            sample_id += 1\n",
    "\n",
    "    # Step 2: Compute baseline generations\n",
    "    print(\"Step 2: Compute baseline generations...\")\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    eval_results = generate_baseline_multihop(mt, df, batch_size=batch_size, max_gen_len=max_gen_len)\n",
    "    for key, value in eval_results.items():\n",
    "        df[key] = list(value)\n",
    "\n",
    "    df.to_csv(os.path.join(fdir_out, \"multihop_product_company_ceo.tsv\"), sep=\"\\t\")\n",
    "    df.to_pickle(os.path.join(fdir_out, \"multihop_product_company_ceo.pkl\"))\n",
    "\n",
    "    correct_subset = df[df[\"is_correct_baseline_hop2\"]].reset_index(drop=True)\n",
    "    correct_subset = correct_subset[correct_subset[\"is_correct_baseline_hop3\"]].reset_index(drop=True)\n",
    "    correct_subset.to_csv(os.path.join(fdir_out, \"multihop_product_company_ceo_only_correct_True.tsv\"), sep=\"\\t\")\n",
    "    correct_subset.to_pickle(os.path.join(fdir_out, \"multihop_product_company_ceo_only_correct_True.pkl\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P1kEXdFap1L",
    "outputId": "e34b34a7-d791-4994-902e-68a7b75b1e5b",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.840784600Z"
    }
   },
   "outputs": [],
   "source": [
    "multihop2_df = generate_CoT_data_prod(batch_size=128 // batch_size_scale, max_gen_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iayMYyIap1L",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.841783800Z"
    }
   },
   "outputs": [],
   "source": [
    "# cot_correct_baseline = run_experiment(\n",
    "#     \"./outputs/preprocessed_data_prod_CoT/factual/multihop_product_company_ceo_only_correct_True.pkl\",\n",
    "#     \"./outputs/results_prod_CoT/factual\",\n",
    "#     fname_out = \"multihop_product_company_ceo_only_correct_True\", batch_size=128 // batch_size_scale, n_samples=-1,\n",
    "#     save_output=True, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAwCFKcsap1L",
    "outputId": "3fdd5376-e0b5-4c34-db08-f1f4a50eeedd",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.842783900Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Base MultiHop Accuracy: \",\n",
    "#       cot_correct_baseline.groupby(['sample_id'])[\"is_correct_baseline_multihop3\"].max().reset_index()[\"is_correct_baseline_multihop3\"].mean())\n",
    "# \n",
    "# print(\"Patching MultiHop Accuracy: \",\n",
    "#       cot_correct_baseline.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\"is_correct_patched\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q40fnAT5ap1L"
   },
   "source": [
    "# Experimet 3: Main CoT experiment\n",
    "\n",
    "This is the full version, using maximal amount of data possible from LRE where a multihop question can be formed combining two single-hop questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njlCL0S_ap1L",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.842783900Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_CoT_data_v7(fname_in=\"./outputs/preprocessed_data_LRE_CoT/factual_multihop/combined_multihop.pkl\",\n",
    "                         fdir_out=\"./outputs/preprocessed_data_LRE_CoT/factual_multihop\",\n",
    "                         batch_size=512 // batch_size_scale, max_gen_len=20):\n",
    "    if not os.path.exists(fdir_out):\n",
    "        os.makedirs(fdir_out)\n",
    "    fname_pkl = f\"{fdir_out}/combined_multihop_CoT_{model_name}_only_correct_True.pkl\"\n",
    "    if os.path.exists(fname_pkl):\n",
    "        print(f\"File {fname_pkl} exists. Skipping...\")\n",
    "        return\n",
    "\n",
    "    print(\"Step 1: Read multihop dataset created using LRE data prep...\")\n",
    "    df = pd.read_pickle(fname_in)\n",
    "\n",
    "    # Step 2: Compute baseline generations\n",
    "    print(\"Step 2: Compute baseline generations...\")\n",
    "    eval_results = generate_baseline_multihop(mt, df, batch_size=batch_size, max_gen_len=max_gen_len)\n",
    "    for key, value in eval_results.items():\n",
    "        df[key] = list(value)\n",
    "\n",
    "    df.to_csv(os.path.join(fdir_out, f\"combined_multihop_CoT_{model_name}.tsv\"), sep=\"\\t\")\n",
    "    df.to_pickle(os.path.join(fdir_out, f\"combined_multihop_CoT_{model_name}.pkl\"))\n",
    "\n",
    "    correct_subset = df[df[\"is_correct_baseline_hop2\"]].reset_index(drop=True)\n",
    "    correct_subset = correct_subset[correct_subset[\"is_correct_baseline_hop3\"]].reset_index(drop=True)\n",
    "    correct_subset.to_csv(os.path.join(fdir_out, f\"combined_multihop_CoT_{model_name}_only_correct_True.tsv\"), sep=\"\\t\")\n",
    "    correct_subset.to_pickle(os.path.join(fdir_out, f\"combined_multihop_CoT_{model_name}_only_correct_True.pkl\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkJ0hUKlap1L",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.843784900Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate_CoT_data_v7(fname_in=\"./outputs/preprocessed_data_LRE_CoT/factual_multihop/combined_multihop.pkl\",\n",
    "#                      fdir_out=\"./outputs/preprocessed_data_LRE_CoT/factual_multihop\",\n",
    "#                      batch_size=128 // batch_size_scale, max_gen_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfRcN9dRap1L",
    "ExecuteTime": {
     "end_time": "2024-06-25T22:29:56.844784Z",
     "start_time": "2024-06-25T22:29:56.844784Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_correct_baseline = run_experiment(\n",
    "    f\"./preprocessed_data/factual_multihop/multihop_CoT_vicuna-13b-v1.1.tsv\",\n",
    "    \"./outputs/preprocessed_data/factual_multihop\",\n",
    "    fname_out=f\"combined_multihop_CoT_{model_name}_only_correct_True\", batch_size=128, n_samples=1000,\n",
    "    save_output=True, replace=False, tsv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLapdFfXap1L",
    "outputId": "c1ec5a9b-489e-440f-d325-bfa37df3d47a",
    "ExecuteTime": {
     "end_time": "2024-06-25T22:29:56.848292200Z",
     "start_time": "2024-06-25T22:29:56.846288100Z"
    }
   },
   "outputs": [],
   "source": [
    "efficient_subset = cot_correct_baseline[\n",
    "    cot_correct_baseline[\"layer_source\"] < cot_correct_baseline[\"layer_target\"]].reset_index(drop=True)\n",
    "# TODO maybe run patching for all source x target, but the killer case is when source < target\n",
    "\n",
    "# print(\"Base MultiHop Accuracy: \",\n",
    "#       cot_correct_baseline.groupby(['sample_id'])[\"is_correct_baseline_multihop3\"].max().reset_index()[\"is_correct_baseline_multihop3\"].mean())\n",
    "\n",
    "print(\"General Patching MultiHop Accuracy (all source layer x target layer): \",\n",
    "      cot_correct_baseline.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\n",
    "          \"is_correct_patched\"].mean())\n",
    "\n",
    "print(\"Efficient Patching MultiHop Accuracy (source layer < target layer): \",\n",
    "      efficient_subset.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\"is_correct_patched\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUO8JFwCoTY-",
    "outputId": "e5344cb7-20ce-4bb5-b545-d2c8bfe12f2d",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.847291700Z"
    }
   },
   "outputs": [],
   "source": [
    "multihop_fname = \"./preprocessed_data/factual_multihop/multihop_CoT_vicuna-13b-v1.1.tsv\"\n",
    "df = pd.read_csv(multihop_fname, sep='\\t', header=0)\n",
    "print(len(df))\n",
    "\n",
    "multihop_fname_only_correct = f\"./outputs/preprocessed_data/factual_multihop/combined_multihop_CoT_{model_name}_only_correct_True.pkl\"\n",
    "df_only_correct = pd.read_pickle(multihop_fname_only_correct)\n",
    "print(len(df_only_correct))\n",
    "df_only_correct.groupby(['fname_src', 'fname_target']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdcYuiPuap1L",
    "ExecuteTime": {
     "end_time": "2024-06-25T22:29:56.849292300Z",
     "start_time": "2024-06-25T22:29:56.848292200Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_patching_heatmaps_from_df(patch_df, _vmin=0, _vmax=None, fname_postfix=\"\", save_output=True):\n",
    "    n_samples = len(patch_df)\n",
    "    plots_dir = \"./outputs/multihop_reasoning\"\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "\n",
    "    # baseline_acc_multihop3 = patch_df[\"is_correct_baseline_multihop3\"].mean()*100\n",
    "    # patching_acc = patch_df.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\"is_correct_patched\"].mean() * 100\n",
    "\n",
    "    heatmap_patched = patch_df.groupby(['layer_target', 'layer_source'])[\"is_correct_patched\"].mean().unstack()\n",
    "\n",
    "    FONT_SIZE_TITLE = 16\n",
    "    FONT_SIZE_AXIS = 15\n",
    "\n",
    "    plt.figure()\n",
    "    ax = sns.heatmap(data=heatmap_patched, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"Self-correction in Multi-hop Reasoning\\n# samples: {n_samples}\", fontsize=FONT_SIZE_TITLE)\n",
    "    plt.xlabel(\"Source Layer ($\\ell$)\", fontsize=FONT_SIZE_AXIS)\n",
    "    plt.ylabel(\"Target Layer ($\\ell^*$)\", fontsize=FONT_SIZE_AXIS)\n",
    "    plt.tight_layout()\n",
    "    if save_output:\n",
    "        fname = f\"{plots_dir}/multihop_heatmap{fname_postfix}.pdf\"\n",
    "        plt.savefig(fname, format=\"pdf\", dpi=300, bbox_inches='tight')\n",
    "        plt.savefig(f\"{fname[:-4]}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTKqZCDQap1L",
    "outputId": "9c9f9970-463a-40b8-af8a-98540b9160c5",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.849292300Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_patching_heatmaps_from_df(efficient_subset, fname_postfix=\"_source_smaller_than_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eG9KXpkap1L",
    "outputId": "f953a771-5302-48d1-9e3e-456c29d811c7",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.850292400Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_patching_heatmaps_from_df(cot_correct_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMKLLwoloTY_"
   },
   "source": [
    "# Experiment 4 - CoT Let's think step by step baseline. Baseline\n",
    "\n",
    "How does a \"Let's think step by step\" CoT baseline compare with the CoT Patchscope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARIDpnXjoTY_",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.851292800Z"
    }
   },
   "outputs": [],
   "source": [
    "def step_by_step_cot_baseline(\n",
    "        fname_in=f\"./preprocessed_data/factual_multihop/combined_multihop_CoT_{model_name}_only_correct_True.pkl\",\n",
    "        fdir_out=\"./outputs/results_CoT/factual_multihop\",\n",
    "        fname_out=f\"combined_multihop_CoT_{model_name}_only_correct_True_step_by_step\",\n",
    "        batch_size=128 // batch_size_scale,\n",
    "        max_gen_len=20,\n",
    "        rewrite=False,\n",
    "        target_col=\"baseline_multihop3\",\n",
    "        object_col=\"hop3\",\n",
    "        cot_prefix=\"Let's think step by step. \"):\n",
    "    if not os.path.exists(fname_in):\n",
    "        print(f'File {fname_in} does not exist. Skipping...')\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(fdir_out):\n",
    "        os.makedirs(fdir_out)\n",
    "    fname_pkl = f\"{fdir_out}/{fname_out}.pkl\"\n",
    "    if rewrite == False and os.path.exists(fname_pkl):\n",
    "        print(f\"\\t{fname_pkl} exists. Skipping generation. Reading file...\")\n",
    "        df = pd.read_pickle(fname_pkl)\n",
    "        return df\n",
    "\n",
    "    print(\"Computing step-by-step baseline generations...\")\n",
    "    df = pd.read_pickle(fname_in)\n",
    "    df[\"cot_prefix\"] = cot_prefix\n",
    "\n",
    "    def _generate_baseline_single_batch(batch_df):\n",
    "        batch_size = len(batch_df)\n",
    "\n",
    "        results = {}\n",
    "        target_baseline_batch = np.array(batch_df[target_col])\n",
    "        target_baseline_batch = np.core.defchararray.add(cot_prefix, target_baseline_batch.astype(str))\n",
    "        object_batch = np.array(batch_df[object_col])\n",
    "\n",
    "        inp_target_baseline = make_inputs(mt.tokenizer, target_baseline_batch, mt.device)\n",
    "        seq_len_target_baseline = len(inp_target_baseline[\"input_ids\"][0])\n",
    "        output_target_baseline_toks = mt.model.generate(\n",
    "            inp_target_baseline[\"input_ids\"],\n",
    "            max_length=seq_len_target_baseline + max_gen_len,\n",
    "            pad_token_id=mt.model.generation_config.eos_token_id,\n",
    "        )[:, seq_len_target_baseline:]\n",
    "        generations_baseline = decode_tokens(mt.tokenizer, output_target_baseline_toks)\n",
    "        generations_baseline_txt = np.array([\" \".join(sample_gen) for sample_gen in generations_baseline])\n",
    "\n",
    "        is_correct_baseline = np.array([\n",
    "            (object_batch[i] in generations_baseline_txt[i] or\n",
    "             object_batch[i].replace(\" \", \"\") in generations_baseline_txt[i].replace(\" \", \"\"))\n",
    "            for i in range(batch_size)\n",
    "        ])\n",
    "        results.update(\n",
    "            {\n",
    "                f\"step_by_step_generations_{target_col}\": generations_baseline_txt,\n",
    "                f\"step_by_step_is_correct_{target_col}\": is_correct_baseline,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    results = {}\n",
    "    n_batches = len(df) // batch_size\n",
    "    if len(df) % batch_size != 0:\n",
    "        n_batches += 1\n",
    "    for i in tqdm.tqdm(range(n_batches)):\n",
    "        cur_df = df.iloc[batch_size * i: batch_size * (i + 1)]\n",
    "        batch_results = _generate_baseline_single_batch(cur_df)\n",
    "        for key, value in batch_results.items():\n",
    "            if key in results:\n",
    "                results[key] = np.concatenate((results[key], value))\n",
    "            else:\n",
    "                results[key] = value\n",
    "\n",
    "    for key, value in results.items():\n",
    "        df[key] = list(value)\n",
    "\n",
    "    df.to_csv(os.path.join(fdir_out, f\"{fname_out}.tsv\"), sep=\"\\t\")\n",
    "    df.to_pickle(os.path.join(fdir_out, f\"{fname_out}.pkl\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "En7LoZD6oTY_",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.852292500Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_correct_baseline_step_by_step_baseline = step_by_step_cot_baseline(\n",
    "    fname_in=f\"./outputs/preprocessed_data/factual_multihop/combined_multihop_CoT_{model_name}_only_correct_True.pkl\",\n",
    "    fdir_out=\"./outputs/results_LRE_CoT/factual_multihop\",\n",
    "    fname_out=f\"combined_multihop_CoT_{model_name}_only_correct_True_step_by_step\",\n",
    "    batch_size=128 // batch_size_scale,\n",
    "    max_gen_len=20,\n",
    "    target_col=\"baseline_multihop3\",\n",
    "    object_col=\"hop3\",\n",
    "    cot_prefix=\"Let's think step by step. \",\n",
    "    rewrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNq74QGeoTY_",
    "outputId": "c7d1f731-13a5-40f8-c3a4-f3cb4a69a154",
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.853291500Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"Base MultiHop Accuracy: \",\n",
    "#       cot_correct_baseline.groupby(['sample_id'])[\"is_correct_baseline_multihop3\"].max().reset_index()[\"is_correct_baseline_multihop3\"].mean())\n",
    "\n",
    "print(\"General Patching MultiHop Accuracy (all source layer x target layer): \",\n",
    "      cot_correct_baseline.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\n",
    "          \"is_correct_patched\"].mean())\n",
    "\n",
    "print(\"Canonical CoT ('Let's think step by step. ') MultiHop Accuracy: \",\n",
    "      cot_correct_baseline_step_by_step_baseline.groupby(['sample_id'])[\n",
    "          \"step_by_step_is_correct_baseline_multihop3\"].max().reset_index()[\n",
    "          \"step_by_step_is_correct_baseline_multihop3\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PeaKrDMoTZC",
    "ExecuteTime": {
     "end_time": "2024-06-25T22:29:56.882277700Z",
     "start_time": "2024-06-25T22:29:56.853291500Z"
    }
   },
   "outputs": [],
   "source": [
    "cot_correct_baseline_step_by_step_baseline['step_by_step_generations_baseline_multihop3']\n",
    "cot_correct_baseline_step_by_step_baseline[\n",
    "    ['baseline_multihop3', 'hop3', 'generations_baseline_multihop3', 'step_by_step_generations_baseline_multihop3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-25T22:29:56.854292200Z"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
